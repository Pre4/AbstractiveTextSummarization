{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVa5nloLlHCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e73a216-0c37-40cb-e18b-20aa536f1600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n3fNq53cDso"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_id = '1K3I8BPQY-CCFZKC2gEyuQBZMW_GjAEFC'\n",
        "link = 'https://drive.google.com/uc?id=1K3I8BPQY-CCFZKC2gEyuQBZMW_GjAEFC'\n",
        "data = pd.read_csv(link)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGXATpsnGwQS"
      },
      "outputs": [],
      "source": [
        "!pip install attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAiXur_0BGvm"
      },
      "outputs": [],
      "source": [
        "import attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnndbqyLSsy5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT_wg5-LFYrd"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRPqwhCsT1en"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vQYpaaHfFNq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "# from keras.io.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTirNd4oZzx2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE4Vvx7qyyK0"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv(\"https://drive.google.com/uc?id=1K3I8BPQY-CCFZKC2gEyuQBZMW_GjAEFC\", sep=',', encoding='utf-8')\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaNMFSu1BBag"
      },
      "outputs": [],
      "source": [
        "data.drop_duplicates(subset=['Text'], inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyyRe3vrEcV5"
      },
      "outputs": [],
      "source": [
        "data.Info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZAmC7CmEpOM"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50n-SdSjgLVn"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"i'd\": \"i would\",\n",
        "    \"i'd've\": \"i would have\",\n",
        "    \"i'll\": \"i will\",\n",
        "    \"i'll've\": \"i will have\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so as\",\n",
        "    # Add more key-value pairs here if needed\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ1rIFxQxx-x",
        "outputId": "bf9cb7e4-f31c-4dfd-d325-11c587c1c04a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kehdY6xVyPVx"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def text_cleaner (text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup (newString, \"1xml\").text\n",
        "    newString =re.sub(r'\\([^)]*\\)','',newString)\n",
        "    newString = re.sub('\"',\"\", newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[a-zA-Z]\", \"\", newString)\n",
        "    newString = re.sub('[m]{2,)', 'mm', newString)\n",
        "    if (num==0):\n",
        "      tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "      tokens= newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:\n",
        "            long_words.append(1)\n",
        "    return (\" \".join(long_words)).strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XBZ1hpb6Y8J"
      },
      "outputs": [],
      "source": [
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYtmAnuj653x",
        "outputId": "20f59932-1226-404f-ba08-dfec078afcc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c__QwpZ79zm"
      },
      "outputs": [],
      "source": [
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIrH3Nd48ohN",
        "outputId": "3198cec2-20af-4205-8e29-724b859467f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_summary[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc5SaZhB8-qL"
      },
      "outputs": [],
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a92RvwOO-zSo"
      },
      "outputs": [],
      "source": [
        "data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcRlXPhe-746"
      },
      "outputs": [],
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVTPfVnO_PRP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "text_word_count=[]\n",
        "summary_word_count=[]\n",
        "for i in data['cleaned_text']:\n",
        "  text_word_count.append(len(i.split()))\n",
        "for i in data['cleaned_summary']:\n",
        "  summary_word_count.append(len(i.split()))\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJZCrWQVEbTS"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in data['cleaned_summary']:\n",
        "  if(len(i.split())<=8):\n",
        "    count += 1\n",
        "  print(count/len(data['cleaned_summary']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TUO3Y6QFMgh"
      },
      "outputs": [],
      "source": [
        "max_text_len=30\n",
        "max_summary_len=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZHCKOv3Fbod"
      },
      "outputs": [],
      "source": [
        "cleaned_text = np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "for i in range(len(cleaned_text)):\n",
        "  if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_test_len):\n",
        "    short_text.append(cleaned_text[i])\n",
        "    short_summary.append(cleaned_summary[i])\n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHM8XVDBg_eZ",
        "outputId": "a2d55298-3b16-4a48-8fba-64207fb3ffc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 summary\n",
            "0  sostokSummary 1eostok\n",
            "1  sostokSummary 2eostok\n",
            "2  sostokSummary 3eostok\n",
            "3  sostokSummary 4eostok\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have the data in a dictionary as follows\n",
        "data = {\n",
        "    'summary': [\"Summary 1\", \"Summary 2\", \"Summary 3\", \"Summary 4\"]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the lambda function to add 'sostok' and 'eostok' to each value in the 'summary' column\n",
        "df['summary'] = df['summary'].apply(lambda x: 'sostok' + x + 'eostok')\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_pDUjYCHSuG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df['summary'] = df['summary'].apply(lambda x : 'sostok'+ x +'eostok')\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNcc3gAcx0uR",
        "outputId": "61925a22-06a5-4a0f-c0b6-8ad83e7f2c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 text\n",
            "0  sostokText 1eostok\n",
            "1  sostokText 2eostok\n",
            "2  sostokText 3eostok\n",
            "3  sostokText 4eostok\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have the data in a dictionary as follows\n",
        "data = {\n",
        "    'text': [\"Text 1\", \"Text 2\", \"Text 3\", \"Text 4\"]\n",
        "}\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply the lambda function to add 'sostok' and 'eostok' to each value in the 'text' column\n",
        "df['text'] = df['text'].apply(lambda x: 'sostok' + x + 'eostok')\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh_zT0Qu8jZ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']), np.array(df['summary']), test_size=0.1, random_state=0, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsFvVfHXrPxR"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyP_cfo5OxLS"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9SvxZ0bWUPn"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts (list(x_tr))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAFvQh_RrZXj"
      },
      "outputs": [],
      "source": [
        "thresh=4\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key, value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if (value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SP6HLvvNsug5"
      },
      "outputs": [],
      "source": [
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
        "x_tr = pad_sequences(x_tr_seq, maxlen=max_text_len, padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "x_voc = x_tokenizer.num_words + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq7BeI-5vSob",
        "outputId": "98d21b0f-b351-4eea-8141-26b4ef2bffe7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_voc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v6CPIfnvwh0"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts (list(y_tr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlLVBWm0ziPM"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CfrZ7ctwlN5"
      },
      "outputs": [],
      "source": [
        "thresh=6\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key, value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if (value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xvh5IN_zy_E"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_val)\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=max_text_len, padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=max_text_len, padding='post')\n",
        "y_voc = y_tokenizer.num_words + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avJapb3X7YVX"
      },
      "outputs": [],
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyP1MVCZYEur"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "  cnt=0\n",
        "  for j in y_tr[i]:\n",
        "    if j!=0:\n",
        "      cnt=cnt+1\n",
        "  if(cnt==1):\n",
        "    ind.append(i)\n",
        "  y_val=np.delete(y_val,ind, axis=0)\n",
        "  x_val=np.delete(x_val,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Nj35Y4HZUYZ"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "enc_emb = Embedding(x_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h3, state_c3 = encoder_lstm1(enc_output2)\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state,decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h,state_c])\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs,decoder_outputs])\n",
        "decoder_concat_input = Concatenate(axis=-1,name='concat_layer')([decoder_outputs,attn_out])\n",
        "decoder_dense = TimeDistributed(Dense(y_voc,activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "model = Model([encoder_inputs,decoder_inputs], decoder_outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Map0Ybvgqv"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up5xynXqH271"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLxIoP-JINHC"
      },
      "outputs": [],
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1],1)[:,1:],epochs=50,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]],y_val.reshape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-oBfzTWJW-z"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfW4q0WnZpmp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras.models import Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGS1FvaqZzLM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWzsUV0YJubN"
      },
      "outputs": [],
      "source": [
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "# Below tensors will hold the states of the previous time step\n",
        "\n",
        "decoder_state_input_h= Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input (shape=(max_text_len, latent_dim))\n",
        "\n",
        "#Get the embeddings of the decoder sequence\n",
        "\n",
        "dec_emb2= dec_emb_layer (decoder_inputs) # To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate (axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDLeUfiZQzOm"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq): #Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "# Generate empty target sequence of length 1.\n",
        "    target_seq= np.zeros((1,1))\n",
        "\n",
        "# Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "\n",
        "    decoded_sentence = ' '\n",
        "\n",
        "    while not stop_condition:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        #Sample a token\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[e, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eostok\"):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        if(sampled_token == 'eostok' or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeroes((1,1))\n",
        "\n",
        "      # Update the target sequence (of length 1).\n",
        "\n",
        "        target_seq[e, e] = sampled_token_index\n",
        "\n",
        "      # Update internal states\n",
        "        e_h, e_c = h c\n",
        "\n",
        "    return decoded_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A1hjrCKTDu8"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newstring=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "          newstring=newstring+reverse_target_word_index[i]+' '\n",
        "    return newstring\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newstring=' '\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "\n",
        "    return newstring\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3cdQ6jZWUAl"
      },
      "outputs": [],
      "source": [
        "for i in range(0,100):\n",
        "    print(\"Review: \", seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\", seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\", decode_sequence (x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}